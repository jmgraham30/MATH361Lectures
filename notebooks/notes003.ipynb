{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using DrWatson;\n",
    "@quickactivate \"NumericalAnalysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditioning and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To supplement these notes, it is recommended that you watch these video lectures on \n",
    "\n",
    "1. [conditioning](https://www.youtube.com/watch?v=a5oQktSURoE&list=PLvUvOH0OYx3BcZivtXMIwP6hKoYv0YvGn&index=2&t=4s)\n",
    "2. [stability](https://www.youtube.com/watch?v=GgEPL3_wlDo&list=PLvUvOH0OYx3BcZivtXMIwP6hKoYv0YvGn&index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we study numerical methods, it is important to assess and control for what can go wrong. In order to do this, we will distinguish between the concepts of **problem** (which can often be described abstractly as a mathematical function) and the method of solution, *i.e.*, an **algorithm** for solving the problem. Even though we have yet to define these terms, we will emphasize from the start that\n",
    "\n",
    "1. **conditioning** is a property of a problem, and \n",
    "2. **stability** is a property of an algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by observing that a problem and an algorithm both provide sources of errors. A problem will typically involve some data, for example, the problem may require the specification of initial conditions, coefficients, or a matrix. When the problem is input into a computer, even before an algorithm is applied a perturbation will occur perhaps due to roundoff error. By how much might the solution change as a result of this type of perturbation to the problem? This is the issue that conditioning addresses. If a problem is highly sensitive to perturbations, then the problem is said to be **ill-conditioned**. If a problem is ill-conditioned, then there is likely no algorithm that will perform well in solving (even approximately) the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, an algorithm will often require the input of some data. For example, an iterative method to approximate the root of polynomial will require the input of an initial guess. For such an algorithm, what happens if the initial guess is off by some number of digits? Will the error decrease after some number of iterations or will it grow? If the error grows, even when our initial guess is but a small perturbation of the exact solution, then our algorithm is said to be **unstable**. Stable algorithms are always preferred over unstable algorithms. \n",
    "\n",
    "In oder to proceed, we need to develop some theoretical concepts around stability and conditioning. Before doing so, let's develop some further intuition for these ideas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developing Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An algorithm designed to solve (perhaps approximately) a problem or class or problems will typically also involve input data. For example, our iterative method\n",
    "\n",
    "$$x_{n+1} = \\frac{1}{2}\\left(x_{n} + \\frac{a}{x_{n}} \\right),$$\n",
    "\n",
    "for approximating $\\sqrt{a}$ requires an initial guess $x_{0}$ to get started. We know that if $x=\\sqrt{a}$, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}\\left(x + \\frac{a}{x} \\right) &= \\frac{1}{2}\\left(\\sqrt{a} + \\frac{a}{\\sqrt{a}} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sqrt{a} + \\sqrt{a} \\right) \\\\\n",
    "&= \\sqrt{a} \\\\\n",
    "&= x.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, $x=\\sqrt{a}$ is said to be a **fixed-point** (a concept we will define precisely and discuss in detail later) of the iteration\n",
    "\n",
    "$$x_{n+1} = \\frac{1}{2}\\left(x_{n} + \\frac{a}{x_{n}} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, if $x_{0} = \\sqrt{a}$ we get an exact solution after one iteration. What happens if $x_{0} = \\sqrt{a + \\epsilon}$, where $\\epsilon$ is a small perturbation? Later we will prove in a more general setting that this algorithm for approximating $\\sqrt{a}$ is stable. \n",
    "\n",
    "Let's examine an unstable algorithm for approximating $\\sqrt{a}$. In the homework, you will be asked to show that $x=\\sqrt{a}$ is a fixed-point for the iteration \n",
    "\n",
    "$$x_{n+1} = x_{n} + x_{n}^{2} - a.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the specific case where $a=2$. Let $x_{0} = 1.414$, note that this initial guess is accurate to four digits. Let's implement this new algorithm in Julia and apply it with this particular initial guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f(x,a) = x + x^2 - a;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4110842528159986\n"
     ]
    }
   ],
   "source": [
    "x = 1.414;\n",
    "x = f(x,2);\n",
    "x = f(x,2);\n",
    "println(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe that after just two iterations, our error has grown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial relative error 0.0003020000000001355\n",
      "Relative error after one iteration 0.004420615727357524\n"
     ]
    }
   ],
   "source": [
    "intial_rel_error = abs(1.414^2 - 2)/abs(2);\n",
    "one_it_rel_error = abs(x^2 - 2)/abs(2);\n",
    "println(\"Initial relative error \", intial_rel_error);\n",
    "println(\"Relative error after one iteration \", one_it_rel_error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore this some more by trying different number of iterations and/or different initial guesses. To do this, we will write a Julia function that iterates $x_{n+1} = x_{n} + x_{n}^{2} - 2$ some specified number of times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unstable_root (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function unstable_root(init_guess,num_its)\n",
    "    x = init_guess;\n",
    "    for i = 1:num_its\n",
    "        x = f(x,2);\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y1 = unstable_root(1.414,10);\n",
    "y2 = unstable_root(1.5,3);\n",
    "y3 = unstable_root(1.5,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at the resulting relative errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.627342646471925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error 0.8032206019588006\n",
      "Relative error 37.04236602783203\n",
      "Relative error 3.4629837042076306e245\n"
     ]
    }
   ],
   "source": [
    "y1_rel_err = abs(y1^2 - 2)/abs(2);\n",
    "y2_rel_err = abs(y2^2 - 2)/abs(2);\n",
    "y3_rel_err = abs(y3^2 - 2)/abs(2);\n",
    "println(\"Relative error \", y1_rel_err);\n",
    "println(\"Relative error \", y2_rel_err);\n",
    "println(\"Relative error \", y3_rel_err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For one last experiment, let's see what happens if we start with $x_{0}$ equal to `sqrt(2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error 1.1102230246251565e-15\n"
     ]
    }
   ],
   "source": [
    "yf = unstable_root(sqrt(2),1);\n",
    "yf_rel_err = abs(yf^2 - 2)/abs(2);\n",
    "println(\"Relative error \", yf_rel_err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Question:** What do you conclude about the algorithm $x_{n+1} = x_{n} + x_{n}^{2} - a$ when applied to  approximate $\\sqrt{2}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's build some intuition regarding conditioning. Suppose we want to add 1 to some real number $x$. We can describe this problem (abstractly) by a function $h(x) = x + 1$. When we solve this on a computer we will have to deal with finite precision arithmetic since $x \\mapsto \\text{fl}(x) = x(1+\\delta)$, where $|\\delta|\\leq \\frac{\\epsilon_{\\text{mac}}}{2}$. Thus, our problem gets perturbed:\n",
    "\n",
    "$$x + 1 \\mapsto x(1+\\delta) + 1.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's compute the resulting relative error:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{|h(x(1+\\delta))-h(x)|}{|x+1|} &= \\frac{|x(1+\\delta) + 1 - (x+1)|}{|x+1|} \\\\\n",
    "&= \\frac{|\\delta x|}{|x+1|},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and this error can be pretty large if $x+1$ is very small. For example, suppose that $x=-1.0012$ and that we round this to $-1.0$. We can compute the relative error for the input as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001198561725928975"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(-1.0-(-1.0012))/abs(-1.0012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's add by 1.0 and compute the relative error, noting that $-1.0 + 1 = 0$ while $-1.0012 + 1 = -0.0012$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((-1.0012 + 1) - (-1.0 + 1))/abs(-1.0012 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see that we have a **loss of significance** due to **subtractive cancellation**. That is, the problem of adding 1 to a number $x$ when $x+1$ is very small is **ill-conditioned**. Regardless of the algorithm used to perform the addition, this loss of signficance can not be avoided. \n",
    "\n",
    "Now let's be a little more formal in our treatment of conditioning and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Theory of Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that we have a (mathematical) problem that can be represented as a function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$. When this problem is treated computationally, the input gets mapped to it's finite precision representation, that is, $x \\mapsto \\tilde{x} = \\text{fl}(x)$. We are interested in the ratio of relative errors:\n",
    "\n",
    "$$\\frac{\\frac{|f(x) - f(\\tilde{x})|}{|f(x)|}}{\\frac{|x-\\tilde{x}|}{|x|}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, using that $\\tilde{x} = \\text{fl}(x) = x(1+\\delta)$, where $|\\delta|\\leq \\frac{\\epsilon_{\\text{mac}}}{2}$ we have that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\frac{|f(x) - f(\\tilde{x})|}{|f(x)|}}{\\frac{|x-\\tilde{x}|}{|x|}} &= \\frac{\\frac{|f(x) - f(x(1+\\delta))|}{|f(x)|}}{\\frac{|x-x(1+\\delta)|}{|x|}} \\\\\n",
    "&= \\frac{\\frac{|f(x) - f(x(1+\\delta))|}{|f(x)|}}{\\frac{|x\\delta|}{|x|}} \\\\\n",
    "&= \\frac{|f(x) - f(x(1+\\delta))|}{|\\delta f(x)|}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Under the ideal situation, $\\delta = 0$. Thus, we ask, what happens as $\\delta \\rightarrow 0$? This leads us to an important definition. Let\n",
    "\n",
    "$$\\kappa_{f}(x) = \\lim_{\\delta \\rightarrow 0}\\frac{|f(x) - f(x(1+\\delta))|}{|\\delta f(x)|}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then $\\kappa_{f}(x)$ is called the **relative condition number** for the problem $f(x)$.  \n",
    "\n",
    "It is often possible to compute $\\kappa_{f}(x)$ without have to take a limit provided that $f$ is differentiable. This fact comes from the following calculation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\kappa_{f}(x) &= \\lim_{\\delta \\rightarrow 0}\\frac{|f(x) - f(x(1+\\delta))|}{|\\delta f(x)|} \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0}\\left|\\frac{f(x(1+\\delta)) - f(x)}{\\delta x} \\frac{x}{f(x)} \\right| \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0}\\left|\\frac{f(x+\\delta x)) - f(x)}{\\delta x} \\frac{x}{f(x)} \\right| \\\\\n",
    "&= \\left|\\frac{xf'(x)}{f(x)} \\right|.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example, consider a generalization of our problem to add one to a real number. That is, let $f(x) = x - c$ where $c\\in \\mathbb{R}$. Note that the problem of adding one is a special case where $c=-1$. Then\n",
    "\n",
    "$$\\kappa_{f}(x) = \\left|\\frac{x}{x - c}\\right|,$$\n",
    "\n",
    "and this will be large if $|x| \\gg |x-c|$. Note that we get something for free here. There is no significant mathematical difference between the operations of addition and subtraction. Furthermore, the expression $\\left|\\frac{x}{x - c}\\right|$ is symmetric in $x$ and $c$ so that if we perturb $c$ instead of $x$ the relative condition number will be $\\left|\\frac{c}{c - x}\\right|$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A problem were the relative condition number is much larger than 1 is typically considered ill-conditioned.**\n",
    "\n",
    "As an exercise, you should analyze the conditioning of the problem of multiplication by a constant $c$. That is, compute $\\kappa_{f}(x)$ for $f(x) = cx$. You will also show in the homework that the problem of evaluating the square root function for an input near 1 is well-conditioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As the course proceeds, we will consider the conditioning of many common problems. For example, when we study numerical methods for solving linear systems $Ax=b$, where $A$ is a matrix, we will see that the condition of the problem is determined by the condition of the matrix $A$ (to be defined later). At this point you should have some significant understanding of what conditioning is all about. Next, let's look at the concept of stability in greater detail.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Theory of Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall (or realize) that an algorithm is a complete specification of how, exactly, to solve a problem; each step of an algorithm must be unambiguously defined and there can be only a finite number of steps. Roughly, an algorithm is stable if it returns results that are about as accurate as the problem and input data. We have already seen that there might be more than one way to solve (or approximate a solution to) a problem. That is, there might be more than one algorithm that can be applied to a particular problem or class of problems. When error in the result of an algorithm exceeds what conditioning can explain, we call the algorithm **unstable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A reasonable question is, how do we assess the stability of an algorithm. A common approach is via the use of **backward error** and backward error analysis. To define this concept, let $f$ be a problem and let $\\tilde{f}$ be an algorithm for computing the problem $f$. If our (exact) data is $x$ and $\\tilde{y} = \\tilde{f}(x)$, and if there is a value $\\tilde{x}$ such that \n",
    "\n",
    "$$f(\\tilde{x}) = \\tilde{y} = \\tilde{f}(x),$$\n",
    "\n",
    "then the quantity \n",
    "\n",
    "$$\\frac{|x-\\tilde{x}|}{|x|},$$\n",
    "\n",
    "is called the relative **backward error**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The point is, if an algorithm always produces small backward errors, then it is stable. \n",
    "\n",
    "We will illustrate this in the context of rootfinding for polynomials. It is convenient to make use of the Julia package [`Polynomials.jl`](https://github.com/JuliaMath/Polynomials.jl). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will define a six-degree polynomial with six roots that we know exaclty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "36.0 &#45; 36.0&#8729;x &#45; 43.0&#8729;x<sup>2</sup> &#43; 44.0&#8729;x<sup>3</sup> &#43; 6.0&#8729;x<sup>4</sup> &#45; 8.0&#8729;x<sup>5</sup> &#43; 1.0&#8729;x<sup>6</sup>"
      ],
      "text/latex": [
       "$36.0 - 36.0\\cdot x - 43.0\\cdot x^{2} + 44.0\\cdot x^{3} + 6.0\\cdot x^{4} - 8.0\\cdot x^{5} + 1.0\\cdot x^{6}$"
      ],
      "text/plain": [
       "Polynomial(36.0 - 36.0*x - 43.0*x^2 + 44.0*x^3 + 6.0*x^4 - 8.0*x^5 + 1.0*x^6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [-2.0,-1,1,1,3,6] # list the roots\n",
    "p = fromroots(r) # construct the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Float64}:\n",
       " -2.0000000000000013\n",
       " -0.999999999999999\n",
       "  0.9999999902778504\n",
       "  1.0000000097221495\n",
       "  2.9999999999999996\n",
       "  5.999999999999992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_computed = sort(roots(p)) # numerically compute and sort the computed roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can easily compute the relative error for each of the computed roots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Float64}:\n",
       " 6.661338147750939e-16\n",
       " 9.992007221626409e-16\n",
       " 9.722149640900568e-9\n",
       " 9.722149529878266e-9\n",
       " 1.4802973661668753e-16\n",
       " 1.3322676295501878e-15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs.(r - r_computed) ./ abs.(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's compute the backward error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "35.99999999999993 &#45; 35.999999999999915&#8729;x &#45; 42.99999999999997&#8729;x<sup>2</sup> &#43; 43.99999999999998&#8729;x<sup>3</sup> &#43; 5.999999999999973&#8729;x<sup>4</sup> &#45; 7.999999999999991&#8729;x<sup>5</sup> &#43; 1.0&#8729;x<sup>6</sup>"
      ],
      "text/latex": [
       "$35.99999999999993 - 35.999999999999915\\cdot x - 42.99999999999997\\cdot x^{2} + 43.99999999999998\\cdot x^{3} + 5.999999999999973\\cdot x^{4} - 7.999999999999991\\cdot x^{5} + 1.0\\cdot x^{6}$"
      ],
      "text/plain": [
       "Polynomial(35.99999999999993 - 35.999999999999915*x - 42.99999999999997*x^2 + 43.99999999999998*x^3 + 5.999999999999973*x^4 - 7.999999999999991*x^5 + 1.0*x^6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_computed = fromroots(r_computed) # for a polynomials using the computed (estimated) roots of the original polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To assess the backward error, take note that the relevant data in rootfinding are the polynomial coefficients. Thus, we will examine the relative error of the two polynomial coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Float64}:\n",
       " 1.973729821555834e-15\n",
       " 2.3684757858670005e-15\n",
       " 6.609699867535816e-16\n",
       " 4.844609562000683e-16\n",
       " 4.440892098500626e-15\n",
       " 1.1102230246251565e-15\n",
       " 0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs.(coeffs(p) - coeffs(p_computed)) ./ abs.(coeffs(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see that, even though there are some computed roots relatively far from the exact values, they are nevertheless the roots of a polynomial with roots very close to the roots of the original polynomial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the next lecture we will begin to study numerical methods for systems of linear equations, beginnning with square linear systems wich corresponds with Chapter 2 of the textbook. If you feel a bit rusty on linear algebra, it is suggested that you study [this matrix algebra review video](https://www.youtube.com/watch?v=bRM3zrzZYg8&list=PLvUvOH0OYx3BcZivtXMIwP6hKoYv0YvGn&index=5). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
