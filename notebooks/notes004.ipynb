{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using DrWatson;\n",
    "@quickactivate \"NumericalAnalysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation for Solving Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now begin our study of numerical methods for linear systems. Recall that a system of linear equations (also called a linear system) with $m$ equations and $n$ unknowns takes the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} &= b_{1} \\\\\n",
    "a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} &= b_{2} \\\\\n",
    "                                                 &\\vdots \\\\\n",
    "a_{m1}x_{1} + a_{m2}x_{2} + \\cdots + a_{mn}x_{n} &= b_{m}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $mn$ values $a_{ij}$ are the given coefficients, the $n$ values $x_{i}$ are the unknowns, and the $m$ values $b_{j}$ are the specified right hand side values. \n",
    "\n",
    "It is convenient to write a linear system in matrix vector notation such as\n",
    "\n",
    "$$Ax = b,$$\n",
    "\n",
    "where $A$ is a coefficient matrix, $x$ and $b$ are column vectors, and $Ax$ means matrix multiplication. To solve a system of linear equations means to find a vector $x$ such that for a given matrix $A$ and vector $b$ the equation $Ax=b$ is satisfied. Solving linear systems is a reoccurring theme in applied mathematics and science. \n",
    "\n",
    "> It has been estimated that the solution of a linear system of equations enters in at some stage in about 75 percent of all scientific problems. Dahlquist and Björck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You should take note that a large portion of a linear algebra course is devoted to the derivation of general criteria for establishing when a linear system $Ax=b$ has a solution, and in that case when the solution is unique. For example, you can view $A$ is a linear transformation on a vector space and then examine the range and nullspace of $A$. The point here is that you are already armed with powerful theoretical tools for studying the well-posedness of the abstract mathematical problem of solving $Ax=b$. This course considers practical algorithms for obtaining a numerical solution to linear systems.    \n",
    "\n",
    "You may have two questions: \n",
    "\n",
    "1. Why does the problem of solving systems of linear equations come up so frequently?\n",
    "2. Why do we need to learn new methods (new as in ones not already covered in a linear algebra course) for solving such problems? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will provide partial answers to these questions now, but as we proceed through the course you will learn much more regarding answers to these questions, especially the second one. \n",
    "\n",
    "As you learn in a linear algebra course, the concept of linearity is abstract and general. Any finite-dimensional linear problem will have a matrix representation and thus can be reduced to solving a system of linear equations. Even infinite-dimensional linear problems may be well-approximated by finite-dimensional problems, typically via some type of discretization (which of course will result in errors that we need to assess and control for). For example, the famous [heat equation](https://en.wikipedia.org/wiki/Heat_equation) is a linear partial differential equation (PDE) that models the heat flow over time and throughout some spatial domain. The corresponding steady-state problem for the heat equation tells one what the temperature distribution is over the spatial domain as time goes to infinity. The solution to the steady-state problem for the heat equation involves solving a so-called [elliptic PDE](https://en.wikipedia.org/wiki/Elliptic_partial_differential_equation) that is linear. Discretizing a linear elliptic PDE results in a (typically large, like $10000 \\times 10000$ or larger) square system of linear equations. A really great video to illustrate aspects of what we are talking about here is [this presentation](https://www.youtube.com/watch?v=rRCGNvMdLEY&t=29s) by the former NFL player [John Urschel](https://en.wikipedia.org/wiki/John_Urschel). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As further motivation, the famous [Schrödinger equation](https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation#:~:text=The%20Schr%C3%B6dinger%20equation%20is%20a,of%20a%20quantum%2Dmechanical%20system.&text=Those%20two%20parameters%20are%20sufficient,Newton's%20law%20is%20Schr%C3%B6dinger's%20equation.) from [quantum mechanics](https://en.wikipedia.org/wiki/Quantum_mechanics) is another example of a linear PDE. Even when a problem is nonlinear, a key step in solving it is to somehow linearize the problem with the hope that the solution to the linearized problem will provide at least a good approximation to the solution of the original nonlinear one, in fact this often turns out to be the case. We will say a little more about this later, but data science, machine learning, and deep learning in particular are also major sources of problems whose solutions involve solving a system of linear equations at one step or another. See the recent book [Linear Algebra and Learning from Data](https://math.mit.edu/~gs/learningfromdata/) for more on the relation between computational linear algebra and data science.  \n",
    "\n",
    "A short answer to question 2 is, as we have already indicated, conditioning and stability matter. Furthermore, when solving large systems of linear equations, efficiency matters and it may even be important to take into account how much memory is required in order to store the problem data on a computer. \n",
    "\n",
    "Hopefully we have convinced you of the relevance of solving linear systems and the need to do so using numerical algorithms. Thus we begin our brief tour through the vast world of numerical linear algebra. To learn more or get a sense of the vastness of this field, see [Matrix Computations](https://jhupbooks.press.jhu.edu/title/matrix-computations) by Golub and Van Loan.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Square Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will begin by studying the problem of solving $Ax=b$ when $A$ is $n\\times n$ so that $A$ has the same number of rows and columns. In this case the linear system is said to be a square linear system. Written out in full, such a system looks like\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} &= b_{1} \\\\\n",
    "a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} &= b_{2} \\\\\n",
    "                                                 &\\vdots \\\\\n",
    "a_{n1}x_{1} + a_{n2}x_{2} + \\cdots + a_{nn}x_{n} &= b_{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1n}\\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\cdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nn} \\end{array}\\right] \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{array} \\right] = \\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "It is advisable to review at this time any linear algebra for which you might be a bit rusty. As a place to start, [this video](https://www.youtube.com/watch?v=bRM3zrzZYg8&list=PLvUvOH0OYx3BcZivtXMIwP6hKoYv0YvGn&index=5) together with Appendix A from the textbook is recommnded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Julia provides many powerful tools for solving systems of linear equations. In particular, the [LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/) makes matrix computations fast and easy. For example, suppose we want to solve the linear system\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cccc} -1.0 & 3.2 & 2.5 & -4.0\\\\ 1.5 & -3.0 & 7.0 & 2.1 \\\\ -3.2 & -9.8 & 7.5 & 13.0 \\\\ -1.0 & 0.0 & 6.9 & -10.0 \\end{array}\\right] \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\end{array} \\right] = \\left[\\begin{array}{c} -1.0 \\\\ 1.0 \\\\ 0.0 \\\\ -1.0 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "We can easily do this in Julia using the **backslash operator** as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra # load the LinearAlgebra.jl package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       "  0.46227392289927544\n",
       " -0.1019010408349391\n",
       " -0.013260324659117384\n",
       "  0.04462298369528145"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [-1.0 3.2 2.5 -4.0;1.5 -3.0 7.0 2.1; -3.2 -9.8 7.5 13.0; -1.0 0.0 6.9 -10.0] # define A\n",
    "b = [-1.0,1.0,0.0,-1.0] # define right hand side vector b\n",
    "x = A\\b # apply backslash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have obtained a solution which can be checked by computing $Ax$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -0.9999999999999999\n",
       "  0.9999999999999998\n",
       "  2.220446049250313e-16\n",
       " -0.9999999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*x # compute matrix/vector multiplication Ax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The vector $b-Ax$ is called the **residual** and should be $0$ when $x$ is a solution. Let's compute the residual for our example system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -1.1102230246251565e-16\n",
       "  2.220446049250313e-16\n",
       " -2.220446049250313e-16\n",
       " -1.1102230246251565e-16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b - A*x # compute the residual vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We obtain a residual that is very close to (in a sense that we will later make precise) but not actually equal to the zero vector. Here is what we now need to discuss:\n",
    "\n",
    "1. What is backslash doing? That is, what is the underlying algorithm(s)? \n",
    "2. There is obviously error, how do we assess and control for this error? \n",
    "\n",
    "You can start to get some insight into backslash by referencing the help documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1m\\\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "\\(x, y)\n",
       "\\end{verbatim}\n",
       "Left division operator: multiplication of \\texttt{y} by the inverse of \\texttt{x} on the left. Gives floating-point results for integer arguments.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> 3 \\ 6\n",
       "2.0\n",
       "\n",
       "julia> inv(3) * 6\n",
       "2.0\n",
       "\n",
       "julia> A = [4 3; 2 1]; x = [5, 6];\n",
       "\n",
       "julia> A \\ x\n",
       "2-element Vector{Float64}:\n",
       "  6.5\n",
       " -7.0\n",
       "\n",
       "julia> inv(A) * x\n",
       "2-element Vector{Float64}:\n",
       "  6.5\n",
       " -7.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "\\(A, B)\n",
       "\\end{verbatim}\n",
       "Matrix division using a polyalgorithm. For input matrices \\texttt{A} and \\texttt{B}, the result \\texttt{X} is such that \\texttt{A*X == B} when \\texttt{A} is square. The solver that is used depends upon the structure of \\texttt{A}.  If \\texttt{A} is upper or lower triangular (or diagonal), no factorization of \\texttt{A} is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n",
       "\n",
       "For rectangular \\texttt{A} the result is the minimum-norm least squares solution computed by a pivoted QR factorization of \\texttt{A} and a rank estimate of \\texttt{A} based on the R factor.\n",
       "\n",
       "When \\texttt{A} is sparse, a similar polyalgorithm is used. For indefinite matrices, the \\texttt{LDLt} factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 0; 1 -2]; B = [32; -4];\n",
       "\n",
       "julia> X = A \\ B\n",
       "2-element Vector{Float64}:\n",
       " 32.0\n",
       " 18.0\n",
       "\n",
       "julia> A * X == B\n",
       "true\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "(\\)(F::QRSparse, B::StridedVecOrMat)\n",
       "\\end{verbatim}\n",
       "Solve the least squares problem $\\min\\|Ax - b\\|^2$ or the linear system of equations $Ax=b$ when \\texttt{F} is the sparse QR factorization of $A$. A basic solution is returned when the problem is underdetermined.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = sparse([1,2,4], [1,1,1], [1.0,1.0,1.0], 4, 2)\n",
       "4×2 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n",
       " 1.0   ⋅\n",
       " 1.0   ⋅\n",
       "  ⋅    ⋅\n",
       " 1.0   ⋅\n",
       "\n",
       "julia> qr(A)\\fill(1.0, 4)\n",
       "2-element Vector{Float64}:\n",
       " 1.0\n",
       " 0.0\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "\\(x, y)\n",
       "```\n",
       "\n",
       "Left division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> 3 \\ 6\n",
       "2.0\n",
       "\n",
       "julia> inv(3) * 6\n",
       "2.0\n",
       "\n",
       "julia> A = [4 3; 2 1]; x = [5, 6];\n",
       "\n",
       "julia> A \\ x\n",
       "2-element Vector{Float64}:\n",
       "  6.5\n",
       " -7.0\n",
       "\n",
       "julia> inv(A) * x\n",
       "2-element Vector{Float64}:\n",
       "  6.5\n",
       " -7.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "\\(A, B)\n",
       "```\n",
       "\n",
       "Matrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n",
       "\n",
       "For rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n",
       "\n",
       "When `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 0; 1 -2]; B = [32; -4];\n",
       "\n",
       "julia> X = A \\ B\n",
       "2-element Vector{Float64}:\n",
       " 32.0\n",
       " 18.0\n",
       "\n",
       "julia> A * X == B\n",
       "true\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "(\\)(F::QRSparse, B::StridedVecOrMat)\n",
       "```\n",
       "\n",
       "Solve the least squares problem $\\min\\|Ax - b\\|^2$ or the linear system of equations $Ax=b$ when `F` is the sparse QR factorization of $A$. A basic solution is returned when the problem is underdetermined.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = sparse([1,2,4], [1,1,1], [1.0,1.0,1.0], 4, 2)\n",
       "4×2 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n",
       " 1.0   ⋅\n",
       " 1.0   ⋅\n",
       "  ⋅    ⋅\n",
       " 1.0   ⋅\n",
       "\n",
       "julia> qr(A)\\fill(1.0, 4)\n",
       "2-element Vector{Float64}:\n",
       " 1.0\n",
       " 0.0\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  \\(x, y)\u001b[39m\n",
       "\n",
       "  Left division operator: multiplication of \u001b[36my\u001b[39m by the inverse of \u001b[36mx\u001b[39m on the left.\n",
       "  Gives floating-point results for integer arguments.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> 3 \\ 6\u001b[39m\n",
       "\u001b[36m  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> inv(3) * 6\u001b[39m\n",
       "\u001b[36m  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A = [4 3; 2 1]; x = [5, 6];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A \\ x\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m    6.5\u001b[39m\n",
       "\u001b[36m   -7.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> inv(A) * x\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m    6.5\u001b[39m\n",
       "\u001b[36m   -7.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  \\(A, B)\u001b[39m\n",
       "\n",
       "  Matrix division using a polyalgorithm. For input matrices \u001b[36mA\u001b[39m and \u001b[36mB\u001b[39m, the\n",
       "  result \u001b[36mX\u001b[39m is such that \u001b[36mA*X == B\u001b[39m when \u001b[36mA\u001b[39m is square. The solver that is used\n",
       "  depends upon the structure of \u001b[36mA\u001b[39m. If \u001b[36mA\u001b[39m is upper or lower triangular (or\n",
       "  diagonal), no factorization of \u001b[36mA\u001b[39m is required and the system is solved with\n",
       "  either forward or backward substitution. For non-triangular square matrices,\n",
       "  an LU factorization is used.\n",
       "\n",
       "  For rectangular \u001b[36mA\u001b[39m the result is the minimum-norm least squares solution\n",
       "  computed by a pivoted QR factorization of \u001b[36mA\u001b[39m and a rank estimate of \u001b[36mA\u001b[39m based\n",
       "  on the R factor.\n",
       "\n",
       "  When \u001b[36mA\u001b[39m is sparse, a similar polyalgorithm is used. For indefinite matrices,\n",
       "  the \u001b[36mLDLt\u001b[39m factorization does not use pivoting during the numerical\n",
       "  factorization and therefore the procedure can fail even for invertible\n",
       "  matrices.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 0; 1 -2]; B = [32; -4];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> X = A \\ B\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   32.0\u001b[39m\n",
       "\u001b[36m   18.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A * X == B\u001b[39m\n",
       "\u001b[36m  true\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  (\\)(F::QRSparse, B::StridedVecOrMat)\u001b[39m\n",
       "\n",
       "  Solve the least squares problem \u001b[35m\\min\\|Ax - b\\|^2\u001b[39m or the linear system of\n",
       "  equations \u001b[35mAx=b\u001b[39m when \u001b[36mF\u001b[39m is the sparse QR factorization of \u001b[35mA\u001b[39m. A basic solution\n",
       "  is returned when the problem is underdetermined.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = sparse([1,2,4], [1,1,1], [1.0,1.0,1.0], 4, 2)\u001b[39m\n",
       "\u001b[36m  4×2 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\u001b[39m\n",
       "\u001b[36m   1.0   ⋅\u001b[39m\n",
       "\u001b[36m   1.0   ⋅\u001b[39m\n",
       "\u001b[36m    ⋅    ⋅\u001b[39m\n",
       "\u001b[36m   1.0   ⋅\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> qr(A)\\fill(1.0, 4)\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   1.0\u001b[39m\n",
       "\u001b[36m   0.0\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?\\ # help documentation on backslash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We highlight the following information from the backslash help file:\n",
    "\n",
    "> For input matrices A and B, the result X is such that A*X == B when A is square. The solver that is used depends upon the structure of A. If A is upper or lower triangular (or diagonal), no factorization of A is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n",
    "\n",
    "What is this about forward and backward substitution and $LU$ factorization? This is what we will take up in the next lecture. For now, let's foreshadow what is to come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Foreshadowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given a matrix $A$, a matrix factorization of $A$ is simply a finite sequence of matrices that multiply together to equal $A$. That is, a matrix factorization of $A$ is a sequence, $A_{1}$, $A_{2}$, $\\ldots$, $A_{N}$, of matrices that satisfy \n",
    "\n",
    "$$A_{1}A_{2}\\cdots A_{N} = A.$$\n",
    "\n",
    "You've already been exposed to matrix factorizations although perhaps not explicitly so. In linear algebra you performed row operations to put a matrix into reduced row echelon form. It is a fact that row operations can be performed by multiplying by special matrices that we will call row operation matrices. Thus, row reduction is the same as matrix factorization. This already suggests that matrix factorizations might be helpful in solving linear systems, a fact that is indeed true. Let's flesh this out a little more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The solution to $Ax=b$ is $x=A^{-1}b$, where $A^{-1}$ is the matrix inverse of $A$, provided the inverse $A^{-1}$ exists. In general it is a lot of work to invert a matrix. However, there are some classes of matrices that are relatively easy to invert. Now suppose that we can factorize a matrix $A$ as $A=LU$ where it is the fortunate case that the matrix factors $L$ and $U$ are easy to invert. Then the system $Ax=b$ is equivalent to the system $LUx=b$. Now we can split $LUx = b$ into two subsystems:\n",
    "\n",
    "1. $Ly=b$ which has solution $y=L^{-1}b$, and\n",
    "  \n",
    "2. $Ux=y$ which has solution $x=U^{-1}y=U^{-1}L^{-1}b$.\n",
    "\n",
    "Now the $x$ we have just found is the solution to the original system $Ax=b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we will do next is, for square matrices $A$, derive a matrix factorization $A=LU$ where $L$ and $U$ are relatively easy to invert (althoug in practice we don't actually compute the inverse). This will actually turn out to be equivalent to Gaussian elimination that you are already familiar with. \n",
    "\n",
    "As an exercise, can you think of a matrix that has a structure such that the corresponding linear system if relatively easy to solve? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
